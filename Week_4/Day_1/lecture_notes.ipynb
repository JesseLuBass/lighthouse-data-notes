{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d2905c4",
   "metadata": {},
   "source": [
    "Dimensionality reduction \n",
    "\n",
    "number of features a dataset has. \n",
    "Dataframe i.e. number of columns \n",
    "\n",
    "Technique for dimensionality reduction: PCA, LDA\n",
    "\n",
    "Feature selection:\n",
    "* filter method \n",
    "* wrapper methods \n",
    "\n",
    "PCA demo\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c1c1cd",
   "metadata": {},
   "source": [
    "Big data tends to come with high dimensional data \n",
    "structured data: excel, columns that mean different things \n",
    "unstructured data: images, language/text, audio \n",
    "    \n",
    "image: features come from every singel pixel. a 224 x 224 x 3 columns for a single image \n",
    "    \n",
    "audio: columns will be the amplitude of the wave \n",
    "\n",
    "language: desribing using vector for words "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f2b520",
   "metadata": {},
   "source": [
    "Why reduce the dimensionality \n",
    "- visualization \n",
    "- imporving model performance\n",
    "- lower computational xomplexity \n",
    "- dimensionality reduction as unsupervised learning \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "534ab22a",
   "metadata": {},
   "source": [
    "Visualization only works in up to 3 dimensions \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b076861",
   "metadata": {},
   "source": [
    "Improving model performance \n",
    "\n",
    "- denoising \n",
    "- disentagled (uncorrelated) features\n",
    "- fewer model parameters\n",
    "- less overfitting "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8668ae62",
   "metadata": {},
   "source": [
    "Lower computational complexity \n",
    "- dimensionality reduction as compression (less memory)\n",
    "- Fewer computations ( smaller matrix multiplication )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f34909b",
   "metadata": {},
   "source": [
    "Dimensionality reduction as unsupervised learning \n",
    "- High-dimensional data often exists on a low dimensional manifolds \n",
    "- by reducing the dimensionality, we might learn the true underlying structure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a7b02aa",
   "metadata": {},
   "source": [
    "Principal Component analysis - applying eigendecomposition to data \n",
    "principal component - natural choice for representing orthogonal slopes and most remainging spread of the data \n",
    "in practice, we keep the principal component that have the most amount of spread/variance for PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca2ad94",
   "metadata": {},
   "source": [
    "Projecting data onto the PCs\n",
    "\n",
    "How do we go from the original coordinate system to the PC coordinate system? \n",
    "\n",
    "project each data point(vector) onto each PC (vector)\n",
    "\n",
    "We can express the jrojection of multiple data points onto multiple PCs using a matrix multiplication \n",
    "matrix multiplication is pretty much doing multiple dot product at once \n",
    "\n",
    "The geometric interpretation of the dot product is a projection of one vector onto another ( i.e. what is A's length along the direction of B )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55cde23",
   "metadata": {},
   "source": [
    "Picking the number of PCs \n",
    "- one strategy: number of PCs up to a certain % of total varianace explained \n",
    "- elbow rule: cut off when there is an abrupt change on the elbow graph \n",
    "\n",
    "\n",
    "Before PCA< data must mbe mean-centered to compute covariance, but the sklearn class do this automatically \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b1e3ec",
   "metadata": {},
   "source": [
    "Scaling dataframe before PCA\n",
    "\n",
    "- PCA finds dimensions with high variance\n",
    "- if some columns of your data have much higher variance, they will dominate the PCs.\n",
    "- The variance of these columns is often ( eg . mm vs m units)\n",
    "- assume each column is equally important by applying standardscaler \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e899583a",
   "metadata": {},
   "source": [
    "### LDA vs PCA\n",
    "\n",
    "Both reduce dimensionality \n",
    "LDA projections:\n",
    "    - minimize intra-class variance\n",
    "    - Maximize inter-class variance\n",
    "\n",
    "- supervised (LDA) vs unsupervised (PCA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400bcd4e",
   "metadata": {},
   "source": [
    "### Feature selection vs dimensionality reduction \n",
    "\n",
    "- Dimensionality reduction: Creates new features that are functions of the original ones ( for PCA, linear combinations of the original ones)\n",
    "- Fewer selection: Removes redundant features and keeps importnat ones\n",
    "- Why ever use feature selection? What your resulting feature set to still be interpretable "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116611ad",
   "metadata": {},
   "source": [
    "### Filter methods \n",
    "- measure relevance of feature by correlation with dependent variable (target)\n",
    "- if feature is correlated with target, keep. Otherwise, discard\n",
    "- Applied before training ML model \n",
    "\n",
    "- Advantages: fast no trainning involved\n",
    "\n",
    "- Disadvantages: ignore feature combinations, keep redundant features \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade4fda6",
   "metadata": {},
   "source": [
    "### Wrapper methods \n",
    "- train ML model with different subsets of feature \n",
    "- If feature improves performance, add/keep it. Otherwise, ignore/remove it \n",
    "- Applied during trainning ML model \n",
    "- Advantages:\n",
    "    - evaluate features in context of others\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c8862a4",
   "metadata": {},
   "source": [
    "### Forward selection wrapper method \n",
    "1. SelectedFeatures= []\n",
    "2. Find F in (allFeatures - SelectedFeatures) that, if added to SelectedFeatures, best improves model performance\n",
    "3. If adding F improved performance more than some threshold, permanently add it to SelectedFeatures and go back to 2 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b53238ea",
   "metadata": {},
   "source": [
    "### Backward elimination wrapper method \n",
    "1. SelectedFeatures = AllFeatures\n",
    "2. Find F in SelectedFeatures that, if removed from SelectedFeatures, decreases model performance the least. \n",
    "3. If removing F decreased performance less than some threshold, permanently remove it from SelectedFeatures and go back to 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53054f75",
   "metadata": {},
   "source": [
    "### Recursive elimination wrapper method\n",
    "1. Pick K, the number of features to select \n",
    "2. Use a model(usually linera model) to assign weights to features\n",
    "( the weights of important features have higher absolute value)\n",
    "3. Remove the feature with the smallest absolute value weight (least important)\n",
    "4. go back to 2 until we only have k features left \n",
    "5. over this process, you will find out what features tend to get eliminated and which features tend to be kept(hopefully)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1687ee2",
   "metadata": {},
   "source": [
    "### Wrapper method tips\n",
    "1. look for implementations ( sklearn has a rfe implementation, for example)\n",
    "2. it is not possible tell which method will work better until you try \n",
    "3. differernt variable selection algorithms may give you a different answers\n",
    "4. different machine learning algorithms with the same variable selection method may give you different answers \n",
    "5. over this process, you will find out what features tend to get eliminated and which features tend to be kept(hopefully)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf82dcb",
   "metadata": {},
   "source": [
    "In practice, if you don't know how many n_components to keep, leave PCA() blank, and judge based on the plot of the cumulative explained variance "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
